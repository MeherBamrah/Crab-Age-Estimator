{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna --quiet\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import zipfile\n",
        "import optuna\n",
        "\n",
        "def run_shell_command(command):\n",
        "    try:\n",
        "        print(f\"Executing: {command}\")\n",
        "        result = subprocess.run(command, shell=True, check=True, capture_output=True, text=True)\n",
        "        print(result.stdout)\n",
        "        if result.stderr:\n",
        "            print(f\"Stderr: {result.stderr}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error executing command: {command}\")\n",
        "        print(f\"Return code: {e.returncode}\")\n",
        "        print(f\"Output: {e.output}\")\n",
        "        print(f\"Stderr: {e.stderr}\")\n",
        "        raise\n",
        "\n",
        "def setup_and_download_kaggle_data():\n",
        "    kaggle_json_path = 'kaggle.json'\n",
        "    if not os.path.exists(kaggle_json_path):\n",
        "        print(f\"'{kaggle_json_path}' not found. Please upload your Kaggle API token file.\")\n",
        "        try:\n",
        "            from google.colab import files\n",
        "            uploaded = files.upload()\n",
        "            if kaggle_json_path not in uploaded:\n",
        "                print(f\"Upload failed or '{kaggle_json_path}' not uploaded. Please try again.\")\n",
        "                return False\n",
        "            print(f\"'{kaggle_json_path}' uploaded successfully.\")\n",
        "        except ImportError:\n",
        "            print(\"Not in Colab environment or file upload failed. Make sure 'kaggle.json' is in the current directory.\")\n",
        "            return False\n",
        "\n",
        "    print(\"Configuring Kaggle API...\")\n",
        "    os.makedirs(os.path.expanduser('~/.kaggle'), exist_ok=True)\n",
        "    run_shell_command(f'cp {kaggle_json_path} {os.path.expanduser(\"~/.kaggle/\")}')\n",
        "    run_shell_command(f'chmod 600 {os.path.expanduser(\"~/.kaggle/kaggle.json\")}')\n",
        "\n",
        "    competition_name = 'weekly-ml-challenge-2'\n",
        "    data_dir = 'crab_data'\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"Downloading data for competition: {competition_name}\")\n",
        "    try:\n",
        "        run_shell_command(f'kaggle competitions download -c {competition_name} -p {data_dir} --force')\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to download data. Check if you've accepted the competition rules on Kaggle: https://www.kaggle.com/competitions/{competition_name}/rules\")\n",
        "        raise e\n",
        "\n",
        "    zip_file_path = os.path.join(data_dir, f'{competition_name}.zip')\n",
        "    if os.path.exists(zip_file_path):\n",
        "        print(f\"Unzipping {zip_file_path}...\")\n",
        "        try:\n",
        "            with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(data_dir)\n",
        "            print(\"Unzipping complete.\")\n",
        "        except zipfile.BadZipFile:\n",
        "            print(f\"Error: The downloaded file {zip_file_path} is not a valid zip file or is corrupted.\")\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during unzipping: {e}\")\n",
        "            return False\n",
        "    else:\n",
        "        print(f\"Error: Zip file {zip_file_path} not found after download attempt.\")\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def engineer_features_combined(df_input):\n",
        "    df = df_input.copy()\n",
        "\n",
        "    length_col = 'Length'\n",
        "    diameter_col = 'Diameter'\n",
        "    height_col = 'Height'\n",
        "    whole_weight_col = 'Weight'\n",
        "    shucked_weight_col = 'Shucked Weight'\n",
        "    viscera_weight_col = 'Viscera Weight'\n",
        "    shell_weight_col = 'Shell Weight'\n",
        "\n",
        "    original_measurement_cols = [length_col, diameter_col, height_col,\n",
        "                                 whole_weight_col, shucked_weight_col,\n",
        "                                 viscera_weight_col, shell_weight_col]\n",
        "\n",
        "    missing_cols = [col for col in original_measurement_cols if col not in df.columns]\n",
        "    if missing_cols:\n",
        "        raise KeyError(f\"Feature Engineering Error: Missing expected original columns: {missing_cols}. \"\n",
        "                       f\"Available columns are: {df.columns.tolist()}. \"\n",
        "                       f\"Please check the column name definitions at the start of 'engineer_features_combined'.\")\n",
        "\n",
        "    df['Approx_Volume'] = df[length_col] * df[diameter_col] * df[height_col]\n",
        "    df['Approx_Density'] = df[whole_weight_col] / (df['Approx_Volume'] + 1e-6)\n",
        "    df['Length_to_Diameter'] = df[length_col] / (df[diameter_col] + 1e-6)\n",
        "    df['Height_to_Diameter'] = df[height_col] / (df[diameter_col] + 1e-6)\n",
        "    df['Meat_Ratio'] = df[shucked_weight_col] / (df[whole_weight_col] + 1e-6)\n",
        "    df['Viscera_to_Whole_Weight'] = df[viscera_weight_col] / (df[whole_weight_col] + 1e-6)\n",
        "    df['Shell_to_Whole_Weight'] = df[shell_weight_col] / (df[whole_weight_col] + 1e-6)\n",
        "    df['Shell_Thickness_Proxy'] = df[shell_weight_col] / (df['Approx_Volume'] + 1e-6)\n",
        "    df['Non_Meat_Weight'] = df[whole_weight_col] - df[shucked_weight_col]\n",
        "    df['Shell_Est_Weight'] = df[whole_weight_col] - df[shucked_weight_col] - df[viscera_weight_col]\n",
        "    df['BMI_like'] = df[whole_weight_col] / (df[height_col]**2 + 1e-6)\n",
        "    df['Sum_Internal_Weights'] = df[shucked_weight_col] + df[viscera_weight_col]\n",
        "\n",
        "    for col_name in original_measurement_cols:\n",
        "        df[f'{col_name}_sq'] = df[col_name]**2\n",
        "\n",
        "    df['Length_Height_Interaction'] = df[length_col] * df[height_col]\n",
        "    df['Weight_Density_Interaction'] = df[whole_weight_col] * df['Approx_Density']\n",
        "\n",
        "    key_features_for_poly = [length_col, diameter_col, height_col, whole_weight_col]\n",
        "\n",
        "    missing_poly_keys = [col for col in key_features_for_poly if col not in df.columns]\n",
        "    if missing_poly_keys:\n",
        "        raise KeyError(f\"Feature Engineering Error (Polynomial): Missing keys for polynomial features: {missing_poly_keys}. \"\n",
        "                       f\"Available columns: {df.columns.tolist()}\")\n",
        "\n",
        "    poly_transformer = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
        "    poly_features_array = poly_transformer.fit_transform(df[key_features_for_poly])\n",
        "    poly_feature_names = poly_transformer.get_feature_names_out(input_features=key_features_for_poly)\n",
        "    new_poly_features_df = pd.DataFrame(poly_features_array, columns=poly_feature_names, index=df.index)\n",
        "    df = pd.concat([df, new_poly_features_df], axis=1)\n",
        "    df = df.loc[:,~df.columns.duplicated(keep='first')]\n",
        "\n",
        "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    for col in df.columns:\n",
        "        if df[col].isnull().any():\n",
        "            df[col] = df[col].fillna(df[col].median())\n",
        "            if df[col].isnull().any():\n",
        "                 df[col] = df[col].fillna(0)\n",
        "    return df\n",
        "\n",
        "X_train_for_optuna = None\n",
        "y_train_for_optuna = None\n",
        "\n",
        "def objective(trial):\n",
        "    global X_train_for_optuna, y_train_for_optuna\n",
        "\n",
        "    params = {\n",
        "        'objective': 'reg:squarederror',\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 200, 2000),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 1.0),\n",
        "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 5.0, log=True),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 5.0, log=True),\n",
        "        'random_state': 42,\n",
        "        'tree_method': 'gpu_hist',\n",
        "        'device': 'cuda'\n",
        "    }\n",
        "\n",
        "    model = xgb.XGBRegressor(**params)\n",
        "    cv_strategy = KFold(n_splits=5, shuffle=True, random_state=trial.number + 42)\n",
        "    scores = cross_val_score(model, X_train_for_optuna, y_train_for_optuna, cv=cv_strategy,\n",
        "                            scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
        "    return -scores.mean()\n",
        "\n",
        "def run_crab_age_prediction_optuna():\n",
        "    global X_train_for_optuna, y_train_for_optuna\n",
        "\n",
        "    data_dir = 'crab_data'\n",
        "    train_file = os.path.join(data_dir, 'train.csv')\n",
        "    test_file = os.path.join(data_dir, 'test.csv')\n",
        "    sample_submission_file = os.path.join(data_dir, 'sample_submission.csv')\n",
        "\n",
        "    if not all(os.path.exists(f) for f in [train_file, test_file, sample_submission_file]):\n",
        "        print(\"One or more data files not found in 'crab_data' directory.\")\n",
        "        return\n",
        "\n",
        "    print(\"Loading data...\")\n",
        "    train_df_orig = pd.read_csv(train_file)\n",
        "    test_df_orig = pd.read_csv(test_file)\n",
        "\n",
        "    print(\"Original train_df columns:\", train_df_orig.columns.tolist())\n",
        "    print(\"Original test_df columns:\", test_df_orig.columns.tolist())\n",
        "\n",
        "    submission_df_template = pd.read_csv(sample_submission_file)\n",
        "    test_ids = test_df_orig['id']\n",
        "\n",
        "    print(\"Preprocessing and Feature Engineering...\")\n",
        "    y_train_for_optuna = train_df_orig['Age']\n",
        "\n",
        "    X_train_raw = train_df_orig.drop(['id', 'Age'], axis=1, errors='ignore')\n",
        "    X_test_raw = test_df_orig.drop('id', axis=1, errors='ignore')\n",
        "\n",
        "    train_len = len(X_train_raw)\n",
        "    combined_df_raw = pd.concat([X_train_raw, X_test_raw], ignore_index=True)\n",
        "\n",
        "    combined_df_processed = pd.get_dummies(combined_df_raw, columns=['Sex'], prefix='Sex', dummy_na=False)\n",
        "    combined_df_engineered = engineer_features_combined(combined_df_processed)\n",
        "\n",
        "    X_train_engineered = combined_df_engineered.iloc[:train_len]\n",
        "    X_test_engineered = combined_df_engineered.iloc[train_len:]\n",
        "\n",
        "    train_cols = X_train_engineered.columns.tolist()\n",
        "    X_test_engineered = X_test_engineered.reindex(columns=train_cols, fill_value=0)\n",
        "    X_train_engineered = X_train_engineered[train_cols]\n",
        "\n",
        "    X_train_for_optuna = X_train_engineered\n",
        "\n",
        "    print(\"Engineered train data shape:\", X_train_for_optuna.shape)\n",
        "    print(\"Engineered test data shape:\", X_test_engineered.shape)\n",
        "    print(f\"Number of features: {X_train_for_optuna.shape[1]}\")\n",
        "\n",
        "    print(\"Starting Hyperparameter Tuning with Optuna for XGBoost (GPU)...\")\n",
        "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "    study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=123))\n",
        "    study.optimize(objective, n_trials=30)\n",
        "\n",
        "    print(\"\\nOptuna Study Statistics: \")\n",
        "    print(\"  Number of finished trials: \", len(study.trials))\n",
        "    print(\"  Best trial:\")\n",
        "    best_trial = study.best_trial\n",
        "    print(\"    Value (RMSE): \", best_trial.value)\n",
        "    print(\"    Params: \")\n",
        "    for key, value in best_trial.params.items():\n",
        "        print(f\"      {key}: {value}\")\n",
        "\n",
        "    best_params = best_trial.params\n",
        "    best_params['objective'] = 'reg:squarederror'\n",
        "    best_params['random_state'] = 42\n",
        "    best_params['tree_method'] = 'gpu_hist'\n",
        "    best_params['device'] = 'cuda'\n",
        "\n",
        "    print(\"\\nTraining final XGBoost model with best parameters on full training data (GPU)...\")\n",
        "    final_xgb_model = xgb.XGBRegressor(**best_params)\n",
        "    final_xgb_model.fit(X_train_for_optuna, y_train_for_optuna)\n",
        "\n",
        "    print(\"Making predictions with the final model...\")\n",
        "    predictions_float = final_xgb_model.predict(X_test_engineered)\n",
        "\n",
        "    min_target_observed = y_train_for_optuna.min()\n",
        "    lower_bound_target = max(1, int(min_target_observed))\n",
        "\n",
        "    predictions_clipped = np.clip(predictions_float, lower_bound_target, None)\n",
        "    predictions_int = np.round(predictions_clipped).astype(int)\n",
        "\n",
        "    print(\"Creating submission file...\")\n",
        "    submission_output_df = pd.DataFrame({\n",
        "        'id': test_ids,\n",
        "        'Yield': predictions_int\n",
        "    })\n",
        "    submission_output_df['id'] = submission_output_df['id'].astype(submission_df_template['id'].dtype)\n",
        "\n",
        "    submission_file_path = 'submission_yield_gpu.csv'\n",
        "    submission_output_df.to_csv(submission_file_path, index=False)\n",
        "    print(f\"Submission file created: {submission_file_path}\")\n",
        "    print(submission_output_df.head())\n",
        "\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        files.download(submission_file_path)\n",
        "        print(f\"'{submission_file_path}' prepared for download.\")\n",
        "    except ImportError:\n",
        "        print(f\"Not in Colab. '{submission_file_path}' is saved in the current directory.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    if setup_and_download_kaggle_data():\n",
        "        run_crab_age_prediction_optuna()\n",
        "    else:\n",
        "        print(\"Failed to setup Kaggle data. ML workflow aborted.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "004h1kXOodw-",
        "outputId": "1bf3f024-477e-4a4c-db78-2f18860a3c7a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuring Kaggle API...\n",
            "Executing: cp kaggle.json /root/.kaggle/\n",
            "\n",
            "Executing: chmod 600 /root/.kaggle/kaggle.json\n",
            "\n",
            "Downloading data for competition: weekly-ml-challenge-2\n",
            "Executing: kaggle competitions download -c weekly-ml-challenge-2 -p crab_data --force\n",
            "Downloading weekly-ml-challenge-2.zip to crab_data\n",
            "\n",
            "\n",
            "Stderr: \n",
            "  0%|          | 0.00/551k [00:00<?, ?B/s]\n",
            "100%|██████████| 551k/551k [00:00<00:00, 739MB/s]\n",
            "\n",
            "Unzipping crab_data/weekly-ml-challenge-2.zip...\n",
            "Unzipping complete.\n",
            "Loading data...\n",
            "Original train_df columns: ['id', 'Sex', 'Length', 'Diameter', 'Height', 'Weight', 'Shucked Weight', 'Viscera Weight', 'Shell Weight', 'Age']\n",
            "Original test_df columns: ['id', 'Sex', 'Length', 'Diameter', 'Height', 'Weight', 'Shucked Weight', 'Viscera Weight', 'Shell Weight']\n",
            "Preprocessing and Feature Engineering...\n",
            "Engineered train data shape: (15000, 37)\n",
            "Engineered test data shape: (10000, 37)\n",
            "Number of features: 37\n",
            "Starting Hyperparameter Tuning with Optuna for XGBoost (GPU)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Optuna Study Statistics: \n",
            "  Number of finished trials:  30\n",
            "  Best trial:\n",
            "    Value (RMSE):  1.9535981624401895\n",
            "    Params: \n",
            "      n_estimators: 814\n",
            "      learning_rate: 0.016080594505164823\n",
            "      max_depth: 4\n",
            "      subsample: 0.928925626790364\n",
            "      colsample_bytree: 0.8798211836331428\n",
            "      gamma: 3.6207922246733614e-07\n",
            "      reg_alpha: 0.3809901992098999\n",
            "      reg_lambda: 4.825132823630822e-07\n",
            "\n",
            "Training final XGBoost model with best parameters on full training data (GPU)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [12:50:40] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Making predictions with the final model...\n",
            "Creating submission file...\n",
            "Submission file created: submission_yield_gpu.csv\n",
            "      id  Yield\n",
            "0  15000     15\n",
            "1  15001     11\n",
            "2  15002      9\n",
            "3  15003     10\n",
            "4  15004     12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [12:50:41] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [12:50:41] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
            "Potential solutions:\n",
            "- Use a data structure that matches the device ordinal in the booster.\n",
            "- Set the device for booster before call to inplace_predict.\n",
            "\n",
            "This warning will only be shown once.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_edf96d2a-7787-4296-8e2b-c2eb07bc4668\", \"submission_yield_gpu.csv\", 85840)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'submission_yield_gpu.csv' prepared for download.\n"
          ]
        }
      ]
    }
  ]
}